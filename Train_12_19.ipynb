{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxZP4LX+KLC5DutRL88clk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import tensorflow"],"metadata":{"id":"qKroKUawKbrA","executionInfo":{"status":"ok","timestamp":1703829708944,"user_tz":-540,"elapsed":5501,"user":{"displayName":"JeongMin Kang","userId":"11670142024132143585"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ibOD7ABg_lDm","executionInfo":{"status":"ok","timestamp":1703829567429,"user_tz":-540,"elapsed":24560,"user":{"displayName":"JeongMin Kang","userId":"11670142024132143585"}},"outputId":"0822bd85-af5f-401a-b1a8-4a00c1068eb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["epochs: 10 losses: 0.44051808714866636\n","epochs: 20 losses: 0.3854432821273804\n","epochs: 30 losses: 0.3705441623926163\n","epochs: 40 losses: 0.3649975091218948\n","epochs: 50 losses: 0.3623473793268204\n","epochs: 60 losses: 0.36081927120685575\n","epochs: 70 losses: 0.3603967994451523\n","epochs: 80 losses: 0.35987205505371095\n","epochs: 90 losses: 0.3598998934030533\n","epochs: 100 losses: 0.3596454530954361\n","[[ 7  2]\n"," [ 1 10]]\n"]}],"source":["import numpy as np\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.functional as F\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","from sklearn.datasets import make_moons\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve,auc\n","\n","class StepByStep(object):\n","    def __init__(self, model, loss_fn, optimizer):\n","        # Here we define the attributes of our class\n","\n","        # We start by storing the arguments as attributes\n","        # to use them later\n","        self.model = model\n","        self.loss_fn = loss_fn\n","        self.optimizer = optimizer\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        # Let's send the model to the specified device right away\n","        self.model.to(self.device)\n","\n","        # These attributes are defined here, but since they are\n","        # not informed at the moment of creation, we keep them None\n","        self.train_loader = None\n","        self.val_loader = None\n","        self.writer = None\n","\n","        # These attributes are going to be computed internally\n","        self.losses = []\n","        self.val_losses = []\n","        self.total_epochs = 0\n","\n","        # Creates the train_step function for our model,\n","        # loss function and optimizer\n","        # Note: there are NO ARGS there! It makes use of the class\n","        # attributes directly\n","        self.train_step = self._make_train_step()\n","        # Creates the val_step function for our model and loss\n","        self.val_step = self._make_val_step()\n","\n","    def to(self, device):\n","        # This method allows the user to specify a different device\n","        # It sets the corresponding attribute (to be used later in\n","        # the mini-batches) and sends the model to the device\n","        self.device = device\n","        self.model.to(self.device)\n","\n","    def set_loaders(self, train_loader, val_loader=None):\n","        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n","        # Both loaders are then assigned to attributes of the class\n","        # So they can be referred to later\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","\n","    def set_tensorboard(self, name, folder='runs'):\n","        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n","        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n","        self.writer = SummaryWriter('{}/{}_{}'.format(\n","            folder, name, suffix\n","        ))\n","\n","    def _make_train_step(self):\n","        # This method does not need ARGS... it can refer to\n","        # the attributes: self.model, self.loss_fn and self.optimizer\n","\n","        # Builds function that performs a step in the train loop\n","        def perform_train_step(x, y):\n","            # Sets model to TRAIN mode\n","            self.model.train()\n","\n","            # Step 1 - Computes our model's predicted output - forward pass\n","            yhat = self.model(x)\n","            # Step 2 - Computes the loss\n","            loss = self.loss_fn(yhat, y)\n","            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n","            loss.backward()\n","            # Step 4 - Updates parameters using gradients and the learning rate\n","            self.optimizer.step()\n","            self.optimizer.zero_grad()\n","\n","            # Returns the loss\n","            return loss.item()\n","\n","        # Returns the function that will be called inside the train loop\n","        return perform_train_step\n","\n","    def _make_val_step(self):\n","        # Builds function that performs a step in the validation loop\n","        def perform_val_step(x, y):\n","            # Sets model to EVAL mode\n","            self.model.eval()\n","\n","            # Step 1 - Computes our model's predicted output - forward pass\n","            yhat = self.model(x)\n","            # Step 2 - Computes the loss\n","            loss = self.loss_fn(yhat, y)\n","            # There is no need to compute Steps 3 and 4,\n","            # since we don't update parameters during evaluation\n","            return loss.item()\n","\n","        return perform_val_step\n","\n","    def _mini_batch(self, validation=False):\n","        # The mini-batch can be used with both loaders\n","        # The argument `validation`defines which loader and\n","        # corresponding step function is going to be used\n","        if validation:\n","            data_loader = self.val_loader\n","            step = self.val_step\n","        else:\n","            data_loader = self.train_loader\n","            step = self.train_step\n","\n","        if data_loader is None:\n","            return None\n","\n","        # Once the data loader and step function, this is the\n","        # same mini-batch loop we had before\n","        mini_batch_losses = []\n","        for x_batch, y_batch in data_loader:\n","            x_batch = x_batch.to(self.device)\n","            y_batch = y_batch.to(self.device)\n","\n","            mini_batch_loss = step(x_batch, y_batch)\n","            mini_batch_losses.append(mini_batch_loss)\n","\n","        loss = np.mean(mini_batch_losses)\n","        return loss\n","\n","    def set_seed(self, seed=42):\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","\n","    def train(self, n_epochs, seed=42):\n","        # To ensure reproducibility of the training process\n","        self.set_seed(seed)\n","\n","        for epoch in range(n_epochs):\n","            # Keeps track of the numbers of epochs\n","            # by updating the corresponding attribute\n","            self.total_epochs += 1\n","\n","            # inner loop\n","            # Performs training using mini-batches\n","            loss = self._mini_batch(validation=False)\n","            if self.total_epochs % 10==0:\n","                print('epochs:',self.total_epochs,'losses:',loss)\n","            self.losses.append(loss)\n","\n","            # VALIDATION\n","            # no gradients in validation!\n","            with torch.no_grad():\n","                # Performs evaluation using mini-batches\n","                val_loss = self._mini_batch(validation=True)\n","                self.val_losses.append(val_loss)\n","\n","            # If a SummaryWriter has been set...\n","            if self.writer:\n","                scalars = {'training': loss}\n","                if val_loss is not None:\n","                    scalars.update({'validation': val_loss})\n","                # Records both losses for each epoch under the main tag \"loss\"\n","                self.writer.add_scalars(main_tag='loss',\n","                                        tag_scalar_dict=scalars,\n","                                        global_step=epoch)\n","\n","        if self.writer:\n","            # Closes the writer\n","            self.writer.close()\n","\n","    def save_checkpoint(self, filename):\n","        # Builds dictionary with all elements for resuming training\n","        checkpoint = {'epoch': self.total_epochs,\n","                      'model_state_dict': self.model.state_dict(),\n","                      'optimizer_state_dict': self.optimizer.state_dict(),\n","                      'loss': self.losses,\n","                      'val_loss': self.val_losses}\n","\n","        torch.save(checkpoint, filename)\n","\n","    def load_checkpoint(self, filename):\n","        # Loads dictionary\n","        checkpoint = torch.load(filename)\n","\n","        # Restore state for model and optimizer\n","        self.model.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","        self.total_epochs = checkpoint['epoch']\n","        self.losses = checkpoint['loss']\n","        self.val_losses = checkpoint['val_loss']\n","\n","        self.model.train() # always use TRAIN for resuming training\n","\n","    def predict(self, x):\n","        # Set is to evaluation mode for predictions\n","        self.model.eval()\n","        # Takes aNumpy input and make it a float tensor\n","        x_tensor = torch.as_tensor(x).float()\n","        # Send input to device and uses model for prediction\n","        y_hat_tensor = self.model(x_tensor.to(self.device))\n","        # Set it back to train mode\n","        self.model.train()\n","        # Detaches it, brings it to CPU and back to Numpy\n","        return y_hat_tensor.detach().cpu().numpy()\n","\n","    def plot_losses(self):\n","        fig = plt.figure(figsize=(10, 4))\n","        plt.plot(self.losses, label='Training Loss', c='b')\n","        plt.plot(self.val_losses, label='Validation Loss', c='r')\n","        plt.yscale('log')\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","        plt.tight_layout()\n","        return fig\n","\n","    def add_graph(self):\n","        # Fetches a single mini-batch so we can use add_graph\n","        if self.train_loader and self.writer:\n","            x_sample, y_sample = next(iter(self.train_loader))\n","            self.writer.add_graph(self.model, x_sample.to(self.device))\n","\n","'''\n","model = nn.Sequential()\n","model.add_module('hidden', nn.Linear(2, 10))\n","model.add_module('activation', nn.ReLU())\n","model.add_module('output', nn.Linear(10, 1))\n","model.add_module('sigmoid', nn.Sigmoid())\n","\n","loss_fn = nn.BCELoss()\n","print(model)\n","'''\n","\n","X,y = make_moons(n_samples=100,noise=0.3, random_state=0)\n","X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=.2,random_state=13)\n","sc = StandardScaler()\n","sc.fit(X_train)\n","X_train = sc.transform(X_train)\n","X_val = sc.transform(X_val)\n","\n","torch.manual_seed(13)\n","\n","x_train_tensor = torch.as_tensor(X_train).float()\n","y_train_tensor = torch.as_tensor(y_train.reshape(-1,1)).float()\n","\n","x_val_tensor = torch.as_tensor(X_val).float()\n","y_val_tensor = torch.as_tensor(y_val.reshape(-1,1)).float()\n","\n","train_dataset = TensorDataset(x_train_tensor,y_train_tensor)\n","val_dataset = TensorDataset(x_val_tensor,y_val_tensor)\n","\n","train_loader = DataLoader(dataset=train_dataset,batch_size=16,shuffle=True)\n","val_loader = DataLoader(dataset=val_dataset,batch_size=16)\n","\n","lr=0.1\n","\n","torch.manual_seed(42)\n","model = nn.Sequential()\n","model.add_module('linear',nn.Linear(2,1))\n","\n","optimizer = optim.SGD(model.parameters(),lr=lr)\n","\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","n_epochs = 100\n","\n","sbs = StepByStep(model,loss_fn,optimizer)\n","sbs.set_loaders(train_loader,val_loader)\n","sbs.train(n_epochs)\n","'''\n","print(model.state_dict())\n","\n","sbs.plot_losses()\n","'''\n","def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","logits_val = sbs.predict(X_val)\n","probabilities_val = sigmoid(logits_val).squeeze()\n","cm_thresh50 = confusion_matrix(y_val,(probabilities_val >= 0.5))\n","print(cm_thresh50)\n"]}]}